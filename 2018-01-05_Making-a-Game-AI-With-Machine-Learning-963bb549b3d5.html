<!DOCTYPE html><html><head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175038637-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      
      gtag('config', 'UA-175038637-1');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Making a Game AI With Machine Learning</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Making a Game AI With Machine Learning</h1>
</header>
<section data-field="subtitle" class="p-summary">
Making a mini game AI using pysc2 and Q learning
</section>
<section data-field="body" class="e-content">
<section name="d153" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="602b" id="602b" class="graf graf--h3 graf--leading graf--title">Making a Game AI With Machine Learning</h3><p name="d496" id="d496" class="graf graf--p graf-after--h3">Making a mini game AI using pysc2 and Q learning</p><figure name="2924" id="2924" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*xGYd76fGrm5sfDrw2zv32g.jpeg" data-width="1560" data-height="878" data-is-featured="true" src="img/1_xGYd76fGrm5sfDrw2zv32g.jpeg"></div><figcaption class="imageCaption">An example of Starcraft II’s most overpowered unit: The Laser Giraffe</figcaption></figure><p name="e5a7" id="e5a7" class="graf graf--p graf-after--figure">To make an AI for a game you need:</p><p name="997e" id="997e" class="graf graf--p graf-after--p">1 — The AI logic, whether this is scripted behavior or artificial intelligence.</p><p name="138b" id="138b" class="graf graf--p graf-after--p">2 — To convert your game world into something your ai can understand and act upon (API-ifying your game).</p><p name="83ef" id="83ef" class="graf graf--p graf-after--p">The goal of this post is to show you the methods for building AL logic, from scripted behavior to methods that can learn almost any task. This should be considered an intro post for those with some programming skills to get into bot building. We will build an AI that can play a Starcraft II minigame. We’re going to use python, numpy, pysc2. Let’s go!</p><h4 name="5fba" id="5fba" class="graf graf--h4 graf-after--p">Setup pysc2</h4><p name="e070" id="e070" class="graf graf--p graf-after--h4">If you want to make a game agent you need to create an interface/api for your game that an AI can use to see, play, and take actions in your game world. We are going to use Starcraft II, specifically an environment released by deepmind and google called pysc2. <a href="https://github.com/deepmind/pysc2#get-starcraft-ii" data-href="https://github.com/deepmind/pysc2#get-starcraft-ii" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">First we need to install Starcraft II</a> (it’s free); I’m using linux so:</p><figure name="eac9" id="eac9" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/6b1caa11fb144041663fd24f8189d235.js"></script></figure><p name="ffbd" id="ffbd" class="graf graf--p graf-after--figure">Make sure you get version 3.17 of the game as newer versions don’t seem to work with some of the pysc2 functions (see <a href="https://github.com/deepmind/pysc2/blob/master/pysc2/run_configs/platforms.py" data-href="https://github.com/deepmind/pysc2/blob/master/pysc2/run_configs/platforms.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">run_configs/platforms</a>). It takes about 7GB of space once unzipped, and to unzip the password is ‘iagreetotheeula’. I had some trouble getting the 3D view to render on ubuntu.</p><p name="26e0" id="26e0" class="graf graf--p graf-after--p">If you use a mac make sure to keep the game installed in the default location (~) and create ‘Maps’, ‘Replays’ folders in the main folder. <a href="https://us.battle.net/account/download/?show=sc2" data-href="https://us.battle.net/account/download/?show=sc2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Use the installer.</a> Now let’s insall pysc2 and pytorch:</p><figure name="1e15" id="1e15" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/8fec55a30939890af2c062181a2991e1.js"></script></figure><p name="c48a" id="c48a" class="graf graf--p graf-after--figure">Now we need to get the sc2 maps we will use as a playground for our agent:</p><p name="7d59" id="7d59" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Get the </strong><a href="https://github.com/deepmind/pysc2/releases/download/v1.2/mini_games.zip" data-href="https://github.com/deepmind/pysc2/releases/download/v1.2/mini_games.zip" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">mini games</strong></a><strong class="markup--strong markup--p-strong"> maps at that link.</strong></p><p name="ad46" id="ad46" class="graf graf--p graf-after--p">For some reason the mini game zip file on pysc2’s github didn’t work on linux. So I unzipped it on my mac and just moved it to my linux machine. Put the mini_games folder into the `Maps` folder in your StarcraftII Installation folder. The minigame maps actually come with pysc2 but who knows if deepmind will continue to do that. Ok now we have all the software and maps let’s program our first agent and examine the details of the pysc2 environment.</p><h4 name="ad73" id="ad73" class="graf graf--h4 graf-after--p">A Implementing a random agent</h4><p name="d8da" id="d8da" class="graf graf--p graf-after--h4">The AI we are going to make is going to be playing the <a href="https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md#movetobeacon" data-href="https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md#movetobeacon" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">move to beacon</a> game. Our AI will control a marine (small combat unit) and move it around to get to the beacon.</p><p name="df13" id="df13" class="graf graf--p graf-after--p">I’m going to make a simple agent that can interact with this environment. It’s going to move around the map randomly:</p><figure name="5ddc" id="5ddc" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/95cf110f185de3d05774101910bfa551.js"></script></figure><p name="76a3" id="76a3" class="graf graf--p graf-after--figure">Here is a video of the above code running; the agent moves randomy around the map:</p><figure name="c477" id="c477" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/5e38988cdb0700feedda6a936aa874ee.js"></script></figure><figure name="7527" id="7527" class="graf graf--figure graf--iframe graf-after--figure"><iframe src="https://www.youtube.com/embed/t37M0wvg9V8?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure><p name="d415" id="d415" class="graf graf--p graf-after--figure">The feature maps look like this:</p><figure name="47fa" id="47fa" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/aQFD9jWR_b4?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="9d50" id="9d50" class="graf graf--p graf-after--figure">Nothing crazy happens here. You can see the main view with a marine (green) and the beacon (grey blue). The marine just moves around randomly like we told it to. The right part of the screen are all the different views our bot can see. The vary from unit types on the screen, to terrain height maps. <a href="https://github.com/yvan/nbsblogs/blob/master/pysc2_tut/post_code1.ipynb" data-href="https://github.com/yvan/nbsblogs/blob/master/pysc2_tut/post_code1.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">To see more code/explanations for this section please see this notebook.</a></p><p name="6ed6" id="6ed6" class="graf graf--p graf-after--p">Another example of feature layers with more readable text:</p><figure name="bbf6" id="bbf6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*l4g0R6Be--YtAVzVTu8Xtw.png" data-width="1600" data-height="900" src="img/1_l4g0R6Be--YtAVzVTu8Xtw.png"></div></figure><h4 name="8486" id="8486" class="graf graf--h4 graf-after--figure">Implementing a scripted AI</h4><p name="924a" id="924a" class="graf graf--p graf-after--h4">Now we want to do something a bit better than random. In the move to beacon minigame the goal is to move to the beacon. We’ll script a bot that does this:</p><figure name="6c01" id="6c01" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/a7fdd3e3315438642d902de7e555f646.js"></script></figure><p name="40be" id="40be" class="graf graf--p graf-after--figure">Here is a video of this bot playing:</p><figure name="1cb9" id="1cb9" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/10907ebc4be5b16ab8ffccc267dd293c.js"></script></figure><p name="ba36" id="ba36" class="graf graf--p graf-after--figure">You can then view the replay in the Starcraft II game client:</p><figure name="a608" id="a608" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/vbsM3koeiVQ?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure><p name="9b4f" id="9b4f" class="graf graf--p graf-after--figure">As you can see the scripted AI plays the beacon game and moves the marine to the beacon. This scripted bot gets an average reward of 25 per episode after running for 105 episodes. This reward is a reflection of how good our bot is at reaching the becaon before the mingame timer is up (120 sec). Any AI we develop should be at least as good as this scripted bot, so a mean score of 25 once trained. Next we will implement an actual AI (one that learns how to play) using reinforcement learning.</p><h4 name="b699" id="b699" class="graf graf--h4 graf-after--p">Implementing a Q Learning AI</h4><p name="a598" id="a598" class="graf graf--p graf-after--h4">This method is a variant of something called ‘Q Learning’ that tries to learn a value called ‘quality’ for every state in the game world and attributes higher quality to states that can lead to more rewards. We create a table (called a Qtable) with all possible states of our game world on the y axis and all possible actions on the x axis. The quality values are stored in this table and tell us what action to take in any possible state. Here is an example of a Qtable:</p><figure name="73ac" id="73ac" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 52px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 7.3999999999999995%;"></div><img class="graf-image" data-image-id="1*PFQg7F193w-V6Febu_mdpQ.png" data-width="1842" data-height="136" src="img/1_PFQg7F193w-V6Febu_mdpQ.png"></div></figure><p name="3c5a" id="3c5a" class="graf graf--p graf-after--figure">So when it has the marine selected but its not at the beacon, state=<code class="markup--code markup--p-code">(1, 0)</code>, our agent learns that moving to the beacon has the highest value (action at index 3) compared to other actions in the same state. When it doesnt have the marine selected and its not at the beacon, state=<code class="markup--code markup--p-code">(0,0)</code>, our agent learns that select the marine has the highest value (action at index 1). When it is one the beacon and it has the marine selected, state=<code class="markup--code markup--p-code">(1,1),</code> doing nothing is valuable.</p><p name="568f" id="568f" class="graf graf--p graf-after--p">When we update the Q Table in the <code class="markup--code markup--p-code">update_qtable</code> function we’re following this formula:</p><figure name="a7bc" id="a7bc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 683px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.3%;"></div><img class="graf-image" data-image-id="1*njwunqxDEYiLzamA15DqWA.gif" data-width="683" data-height="77" src="img/1_njwunqxDEYiLzamA15DqWA.gif"></div></figure><p name="d114" id="d114" class="graf graf--p graf-after--figure">It basically says ‘take our estimate of the reward of taking an action and compare it against actually taking the action.’ Then take this difference and adjust our Q values to be a little less wrong. Our AI will take state information and spit out an action to take. I‘ve simplified this world state and the actions to make it easier to learn the Q table and keep the code concise. Instead of hardcoding the logic to always move to the beacon we give our agent a choice. We gave it 6 things it can do:</p><p name="8999" id="8999" class="graf graf--p graf-after--p">`_NO_OP` — do nothing</p><p name="0faf" id="0faf" class="graf graf--p graf-after--p">`_SELECT_ARMY` — select the marine</p><p name="8483" id="8483" class="graf graf--p graf-after--p">`__SELECT_POINT` — deselect the marine</p><p name="b1ab" id="b1ab" class="graf graf--p graf-after--p">`_MOVE_SCREEN` — move to the beacon</p><p name="09cc" id="09cc" class="graf graf--p graf-after--p">`_MOVERAND` — move to a random point that is not the beacon</p><p name="9fb9" id="9fb9" class="graf graf--p graf-after--p">`_MOVE_MIDDLE` — move to a point that is in the middle of the map</p><p name="ce36" id="ce36" class="graf graf--p graf-after--p">Here is our Q table code for a pretrained agent:</p><figure name="dd00" id="dd00" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/1010d04115939056c40ae8b1ce18bce2.js"></script></figure><p name="c93f" id="c93f" class="graf graf--p graf-after--figure">Run this agent with pretrained code by downloading the two files (agent3_qtable.npy, agent3_states.npy):</p><figure name="2d57" id="2d57" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvan/564d890d3a1856aa85d18ffdfaeb8839.js"></script></figure><p name="b361" id="b361" class="graf graf--p graf-after--figure">This AI can match the reward of 25 per episode our scripted AI can get once it is trained. It tries a lot of different movements around the map and noticed that in states where the marine and beacon positions are on top of each other it gets a reward. It then tries to take actions in every state that lead to this outcome to maximize reward. Here is a video of this AI playing early on:</p><figure name="aa6a" id="aa6a" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/hsHbpk7n8WU?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure><p name="9eec" id="9eec" class="graf graf--p graf-after--figure">Here is a video once it learns that moving to the beacon provides a reward:</p><figure name="bea8" id="bea8" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/k7yUiPlt96U?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure><p name="ca9a" id="ca9a" class="graf graf--p graf-after--figure">You can also train your own agent with the <a href="https://github.com/yvan/nbsblogs/blob/master/pysc2_tut/post_code3.ipynb" data-href="https://github.com/yvan/nbsblogs/blob/master/pysc2_tut/post_code3.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">notebook code</a>.</p><h4 name="b420" id="b420" class="graf graf--h4 graf-after--p">Conclusion and Future Posts</h4><p name="a8a5" id="a8a5" class="graf graf--p graf-after--h4">In this post I wanted to show you three types of programmed AI behavior, random, scripted, and Q learning agent.</p><p name="6ff6" id="6ff6" class="graf graf--p graf-after--p">As Martin Anward of Paradox development <a href="https://twitter.com/Martin_Anward/status/722145900999667715" data-href="https://twitter.com/Martin_Anward/status/722145900999667715" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">says</a>: “Machine learning for complex games is mostly science fiction at this point in time.” I agree with Martin on several of his points. Still I think there’s potential for machine learning in games. The rest of the thread is how to make a good AI using a weighted lists; neural nets are the same thing as a weighted list — but learned. What’s difficult, and Martin is right here, is that the level of complex interactions is difficult for a computer to piece together and reason about.</p><p name="9624" id="9624" class="graf graf--p graf-after--p">Here is our Q learning beacon AI again, playing on normal speed this time:</p><figure name="1566" id="1566" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/B0Ud7eP8ELE?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure><p name="f231" id="f231" class="graf graf--p graf-after--figure">In this post I stuck with a mini game because I wanted something that was easy to experiment and program around. pysc2 is complicated enough to get working. We still haven’t trained our Q learning agent to identify and move to the beacon (we just give it move to beacon as an option). This is possible but beyond the scope of this introductory post. In the future we’ll do a DQN like <a href="https://arxiv.org/abs/1708.04782" data-href="https://arxiv.org/abs/1708.04782" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Deepmind’s paper</a> and maybe address this more complicated task. Be sure to subscribe to <em class="markup--em markup--p-em">Generation Machine </em>below. <a href="https://github.com/yvan/nbsblogs/tree/master/pysc2_tut" data-href="https://github.com/yvan/nbsblogs/tree/master/pysc2_tut" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Post code and notebooks here.</a></p><p name="f75b" id="f75b" class="graf graf--p graf-after--p graf--trailing"><a href="https://tinyletter.com/generationmachine" data-href="https://tinyletter.com/generationmachine" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">If you are interested in machine learning and video games, check out my newsletter: <em class="markup--em markup--p-em">Generation Machine</em></strong></a><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">.</em></strong></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@yvanscher" class="p-author h-card">Yvan Scher</a> on <a href="https://medium.com/p/963bb549b3d5"><time class="dt-published" datetime="2018-01-05T01:38:33.053Z">January 5, 2018</time></a>.</p><p><a href="https://medium.com/@yvanscher/making-a-game-ai-with-deep-learning-963bb549b3d5" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on June 23, 2020.</p></footer></article></body></html>
